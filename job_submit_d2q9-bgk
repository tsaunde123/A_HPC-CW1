#!/bin/bash

#SBATCH --job-name d2q9-bgk
#SBATCH --nodes 2
#SBATCH --ntasks-per-node 2
#SBATCH --time 00:05:00
#SBATCH --gres=gpu:2
#SBATCH --partition gpu_veryshort
##SBATCH --partition test #cpu
#SBATCH --output d2q9-bgk.out
#SBATCH --exclusive
module load CUDA/8.0.44

echo Running on host `hostname`
echo Time is `date`
echo Directory is `pwd`
echo Slurm job ID is $SLURM_JOB_ID
echo This job runs on the following machines:
echo `echo $SLURM_JOB_NODELIST | uniq`
echo Number of nodes: $SLURM_NNODES
echo CPUs per node: $SLURM_JOB_CPUS_PER_NODE

#! Run the parallel MPI executable (nodes*ppn)
#mpirun ./d2q9-bgk input_128x128.params obstacles_128x128.dat
mpirun -np 1 -env CUDA_VISIBLE_DEVICES 0 ./d2q9-bgk input_128x128.params obstacles_128x128.dat : -np 1 -env CUDA_VISIBLE_DEVICES 1 ./d2q9-bgk input_128x128.params obstacles_128x128.dat : -np 1 -env CUDA_VISIBLE_DEVICES 0 ./d2q9-bgk input_128x128.params obstacles_128x128.dat : -np 1 -env CUDA_VISIBLE_DEVICES 1 ./d2q9-bgk input_128x128.params obstacles_128x128.dat
#nvprof --print-gpu-trace ./d2q9-bgk
#--print-api-trace
